apiVersion: batch/v1
kind: Job
metadata:
  generateName: ${USER}-job-train-surrogate-${JOB_SUFFIX}
  labels:
    eidf/user: ${USER}
    kueue.x-k8s.io/queue-name: ${KUBE_USER_QUEUE}
    kueue.x-k8s.io/priority-class: batch-workload-priority
spec:
  completions: 1
  parallelism: 1
  completionMode: Indexed
  backoffLimit: 2147483647
  activeDeadlineSeconds: 864000
  template:
    metadata:
      labels:
        eidf/user: ${USER}
    spec:
      restartPolicy: OnFailure
      # Replace nodeSelector with affinity to allow multiple GPU types.
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: nvidia.com/gpu.product
                    operator: In
                    values:
                      - NVIDIA-A100-SXM4-80GB
      # Add tolerations to allow scheduling on nodes with specific taints.
      tolerations:
        - key: "eidf107"
          operator: "Equal"
          value: "True"
          effect: "NoSchedule"
      containers:
        - name: crack-in-the-bark
          image: linjunhua3/crack-bark:latest  # Ensure CUDA version matches host drivers
          workingDir: "/workspace/kubernets"
          env:
            - name: PYTHONPATH
              value: "/workspace/kubernets"
          command: ["/bin/bash", "-c"]
          args:
            - |
              echo "Setting up environment..."
              git clone "https://github.com/JunhuaL/crack-in-the-bark.git" .
              git checkout feature/port-over
              
              scp -r /workspace/data_storage/fixed_imagenet /workspace/kubernets/
              scp -r /workspace/data_storage/logs /workspace/kubernets/
              scp -r /workspace/data_storage/output /workspace/kubernets/

              curl -o 512x512_diffusion.pt "https://openaipublic.blob.core.windows.net/diffusion/jul-2021/512x512_diffusion.pt"

              echo "Starting training..."
              export PYTHONPATH=$PYTHONPATH:/workspace/kubernets

              python remove_watermark.py ./logs/run-20250518_005930-imagenet_data-maE1/media/wm_img/ \
                                         ./outputs/models/wm_vs_pub_with_fft.pth \
                                         ./outputs/images/wm_vs_pub_with_fft-ep32/ \
                                         --batch_size=4 \
                                         --init_steps=20 \
                                         --n_steps=200 \
                                         --eps=32 \
                                         --lr=0.5 \
                                         --vae=none \
                                         --apply_fft

              python assess_images.py --run_name=wm_vs_pub_fft_eps_32 \
                                      --original_images_path=./logs/run-20250518_005930-imagenet_data-maE1/media/wm_img/ \
                                      --adv_images_path=./output/images/wm_vs_pub_with_fft-ep32/ \
                                      --table_path=./logs/run-20250518_005930-imagenet_data-maE1/media/table/metadata.csv \
                                      --imagenet_path=./fixed_imagenet/ \
                                      --watermark_path=./logs/run-20250518_005930-imagenet_data-maE1/tr_params.pth \
                                      --model_id=512x512_diffusion 

              scp -r output /workspace/data_storage/
              scp -r logs /workspace/data_storage/

              exit_status=$?
              echo "Training finished with exit status $exit_status"
          resources:
            limits:
              nvidia.com/gpu: "1"
              cpu: "8"
              memory: "64Gi"
          volumeMounts:
            - name: datastorage
              mountPath: /workspace/data_storage
      volumes:
        - name: datastorage
          persistentVolumeClaim:
            claimName: ${USER}-ws2